{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "printable-rwanda",
   "metadata": {},
   "source": [
    "# Inteligência Artificial - Trabalho\n",
    "- Universidade Federal de Santa Catarina\n",
    "- Departamento de Automação e Sistemas\n",
    "- Prof. Eric Aislan Antonelo\n",
    "\n",
    "### Grupo\n",
    "- Yohannes\n",
    "- Gustavo Albino\n",
    "- David Sand\n",
    "\n",
    "### Opção: 1\n",
    "\n",
    "#### 1. Implementação:\n",
    "- Crie as estruturas de dados para guardar os pesos que definem uma arquitetura\n",
    "de rede neural multi-camadas. Inicialize a rede neural aleatoriamente.\n",
    "- Implemente o algoritmo da retropropagação para o cálculo do gradiente, a\n",
    "derivada parcial da função de custo com relação aos pesos da rede.\n",
    "- Valide o algoritmo do cálculo do gradiente, realizando uma aproximação numérica\n",
    "do mesmo. Verifique se os cálculos batem um com o outro.\n",
    "- Dado o gradiente já calculado, implemente o método do descenso do gradiente\n",
    "para o treinamento da rede neural, ou seja, o processo de ajuste dos pesos.\n",
    "\n",
    "#### 2. Aplicação:\n",
    "- Use o código implementado para treinar uma rede neural para realizar a classificação de um padrão de duas dimensões de entrada. Os dados para treinamento\n",
    "estão disponíveis no arquivo\n",
    "classification2.txt.\n",
    "Para plotar a fronteira de decisão da rede treinada, poderá usar o código\n",
    "disponível no link\n",
    "https://colab.research.google.com/drive/1XTtZGgpAefbiWejTrEjsnWzS_\n",
    "XXYdzff?usp=sharing.\n",
    "- Relate resultados variando pelo menos duas vezes cada um dos hiperparâmetros: o número de camadas; o número de neurônios por camada; taxa de aprendizagem. Use métricas como taxa de classificação (porcentagem de predições\n",
    "corretas) no conjunto de validação (exemplos não usados no treinamento).\n",
    "- (opcional) Treine uma rede neural para classificar dígitos a partir de imagens\n",
    "como entrada para a rede. Use o arquivo\n",
    "classification3.mat.\n",
    "\n",
    "#### 3. Entregas:\n",
    "No relatório a ser entregue, descreva os experimentos e os resultados obtidos.\n",
    "Grave um video de até 3 minutos, onde você deve explicar o código implementado\n",
    "de uma forma geral, as dificuldades encontradas, e em especial:\n",
    "- a parte do código referente ao cálculo do gradiente\n",
    "- a parte do código referente ao gradient descent\n",
    "- o gráfico da fronteira de decisão\n",
    "Entregue o código, PDF do relatório e o arquivo de video pelo Moodle (zipado com\n",
    "ZIP ou tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "vital-linux",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import logging\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from numpy import genfromtxt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(message)s')\n",
    "\n",
    "stdout_handler = logging.StreamHandler(sys.stdout)\n",
    "stdout_handler.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(stdout_handler)\n",
    "\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-shanghai",
   "metadata": {},
   "source": [
    "### Data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "agricultural-outdoors",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 118 entries, 0 to 117\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   column1  118 non-null    float64\n",
      " 1   column2  118 non-null    float64\n",
      " 2   label    118 non-null    int64  \n",
      "dtypes: float64(2), int64(1)\n",
      "memory usage: 2.9 KB\n",
      "\n",
      "Label Value counts: \n",
      "0    60\n",
      "1    58\n",
      "Name: label, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column1</th>\n",
       "      <th>column2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.051267</td>\n",
       "      <td>0.69956</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.092742</td>\n",
       "      <td>0.68494</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.213710</td>\n",
       "      <td>0.69225</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.50219</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.513250</td>\n",
       "      <td>0.46564</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    column1  column2  label\n",
       "0  0.051267  0.69956      1\n",
       "1 -0.092742  0.68494      1\n",
       "2 -0.213710  0.69225      1\n",
       "3 -0.375000  0.50219      1\n",
       "4 -0.513250  0.46564      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('classification2.txt', header=None)\n",
    "df.columns = ['column1', 'column2', 'label']\n",
    "df.info()\n",
    "print(f'\\nLabel Value counts: \\n{df.label.value_counts()}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-amino",
   "metadata": {},
   "source": [
    "### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "proprietary-auckland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "corr = df.corr()\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "f, ax = plt.subplots(figsize=(5, 5))\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "sns.heatmap(\n",
    "    corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "    square=True, linewidths=.2, cbar_kws={\"shrink\": .5}\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternate-memorial",
   "metadata": {},
   "source": [
    "### Neural net data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "chinese-monkey",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To ADD / REMOVE bias\n",
    "#self.input_length = input_length# + 1\n",
    "\n",
    "#biased_input_data = np.ones((input_data.shape[0], input_data.shape[1] + 1))\n",
    "#biased_input_data[:, :-1] = input_data\n",
    "\n",
    "#next_activation = np.append(next_activation, 1)\n",
    "\n",
    "#previous_deltas = new_deltas[:-1] # Exclude bias delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dominant-universal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x)) \n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    x[x <= 0] = 0\n",
    "    x[x > 0] = 1\n",
    "    return x\n",
    "\n",
    "class Neuron():\n",
    "    def __init__(self, input_length):\n",
    "        self.__weights = self.create_random_weights(input_length)\n",
    "        \n",
    "    def create_random_weights(self, input_length):\n",
    "        return np.random.uniform(-0.5, 0.5, input_length)\n",
    "        \n",
    "    @property\n",
    "    def weights(self):\n",
    "        return self.__weights\n",
    "    \n",
    "    @weights.setter\n",
    "    def weights(self, new_value):\n",
    "        self.__weights = new_value\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'Neuron {self.weights}'\n",
    "    \n",
    "class Layer():\n",
    "    def __init__(self, input_length, n_neurons):\n",
    "        self.input_length = input_length + 1 # input_length + bias_length\n",
    "        self.n_neurons = n_neurons\n",
    "        self.create_neurons()\n",
    "    \n",
    "    def create_neurons(self):\n",
    "        \"\"\"Creates neurons of layer\n",
    "        \"\"\"\n",
    "        self.neurons = list()\n",
    "        for i in range(self.n_neurons):\n",
    "            self.neurons.append(\n",
    "                Neuron(self.input_length)\n",
    "            )\n",
    "            \n",
    "    def __repr__(self):\n",
    "        return f'Layer \\n{self.weights_matrix}'\n",
    "    \n",
    "    @property\n",
    "    def weights_matrix(self):\n",
    "        \"\"\"Represents layer's weights as a matrix, where each column is a neuron.\n",
    "        \"\"\"\n",
    "        return np.array([n.weights for n in self.neurons])\n",
    "    \n",
    "    @weights_matrix.setter\n",
    "    def weights_matrix(self, new_weights):\n",
    "        for neuron_i, neuron in enumerate(self.neurons):\n",
    "            neuron.weights = new_weights[neuron_i]\n",
    "            \n",
    "    def transform(self, input_data, activation_function):\n",
    "        \"\"\"Calculates layer output based on input data\n",
    "        \"\"\"\n",
    "        \n",
    "        # Include bias\n",
    "        biased_input_data = np.ones((input_data.shape[0], input_data.shape[1] + 1))\n",
    "        biased_input_data[:, :-1] = input_data\n",
    "        \n",
    "        self.z = np.dot(biased_input_data, self.weights_matrix.T)\n",
    "\n",
    "        self.activation = activation_function(self.z)\n",
    "        return self.activation\n",
    "\n",
    "class NeuralNet():\n",
    "    def __init__(self, *layers, learning_rate=0.1, epochs=5,\n",
    "                 activation_function=relu, der_activation_function=relu_derivative,\n",
    "                 last_layer_activation_function=sigmoid, last_layer_der_activation_function=sigmoid_derivative,\n",
    "                ):\n",
    "        self.layers = layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.costs = list()\n",
    "        self.epochs = epochs\n",
    "        self.activation_function = activation_function\n",
    "        self.der_activation_function = der_activation_function\n",
    "        self.last_layer_activation_function = last_layer_activation_function\n",
    "        self.last_layer_der_activation_function = last_layer_der_activation_function\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        for layer in self.layers[:-1]:\n",
    "            X_transformed = layer.transform(X_transformed, activation_function=self.activation_function)\n",
    "\n",
    "        X_transformed = self.layers[-1].transform(\n",
    "            X_transformed,\n",
    "            activation_function=self.last_layer_activation_function\n",
    "        )\n",
    "        return X_transformed\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'Neural Net \\n{self.weights_matrix}'\n",
    "    \n",
    "    @property\n",
    "    def weights_matrix(self):\n",
    "        \"\"\"Represents layer's weights as a matrix, where each column is a neuron.\n",
    "        \"\"\"\n",
    "        return np.array([l.weights_matrix for l in self.layers], dtype=object)\n",
    "     \n",
    "    @property\n",
    "    def der_weights_matrix(self):\n",
    "        \"\"\"Represents layer's derivative weights as a matrix, where each column is a neuron.\n",
    "        \"\"\"\n",
    "        return np.array([l.weights_der for l in self.layers], dtype=object)\n",
    "    \n",
    "    @weights_matrix.setter\n",
    "    def weights_matrix(self, new_weights):\n",
    "        for layer_i, layer in enumerate(self.layers):\n",
    "            layer.weights_matrix = new_weights[layer_i]\n",
    "    \n",
    "    def cost(self, X, y):\n",
    "        hx = self.transform(X)\n",
    "        \n",
    "        log1_hx = np.log(hx)\n",
    "        log2_hx = np.log(1 - hx)\n",
    "        \n",
    "        y_1 = y * log1_hx\n",
    "        y_0 = (1 - y) * log2_hx\n",
    "\n",
    "        return -np.sum(\n",
    "            y_1 + y_0\n",
    "        ) / X.shape[0]\n",
    "    \n",
    "    def has_next_item(self, list_, index_):\n",
    "        return (index_ + 1) != len(list_)\n",
    "\n",
    "    def backpropagate(self, X, hx, y):\n",
    "        loss_der =  np.divide(1 - y, np.maximum(1 - hx, 0.00001)) - np.divide(y, np.maximum(hx, 0.00001))\n",
    "        \n",
    "        deltas = self.last_layer_der_activation_function(hx) * loss_der\n",
    "        \n",
    "        reversed_layers = self.layers[::-1]\n",
    "        for layer_i, layer in enumerate(reversed_layers):\n",
    "            if layer_i > 0:\n",
    "                deltas = np.dot(\n",
    "                    deltas,\n",
    "                    reversed_layers[layer_i-1].weights_matrix[:, :-1],\n",
    "                ) * self.der_activation_function(next_activation)\n",
    "                \n",
    "            if self.has_next_item(reversed_layers, layer_i):\n",
    "                next_activation = reversed_layers[layer_i + 1].activation\n",
    "            else:\n",
    "                next_activation = X\n",
    "        \n",
    "            next_activation_biased = np.append(\n",
    "                next_activation,\n",
    "                np.ones(\n",
    "                    next_activation.shape[0]\n",
    "                ).reshape((next_activation.shape[0], 1)),\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "            layer.weights_der = np.dot(next_activation_biased.T, deltas)\n",
    "            \n",
    "    def update_weights(self):\n",
    "        for layer in self.layers:\n",
    "            layer.weights_matrix -= self.learning_rate * layer.weights_der.T / len(X)\n",
    "            \n",
    "    def plot_cost(self):\n",
    "        plt.plot(self.costs)\n",
    "        plt.title('Cross entrophy cost')\n",
    "        plt.ylabel('Cost')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.show()\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        logger.info(f'Fitting')\n",
    "        for epoch in range(self.epochs):\n",
    "            hx = self.transform(X)\n",
    "            loss = self.cost(X, y)\n",
    "            self.backpropagate(X, hx, y)\n",
    "            self.update_weights()\n",
    "            self.costs.append(loss)\n",
    "            \n",
    "            if epoch % 500 == 0:\n",
    "                logger.info(f'Running epoch {epoch}')\n",
    "                self.plot_cost()\n",
    "                plot_boundary_frontier(X, y.ravel(), title=\"Boundary Frontier for epoch {}\".format(epoch))\n",
    "            \n",
    "        self.plot_cost()\n",
    "        plot_boundary_frontier(X, y.ravel(), title=\"Boundary Frontier for epoch {}\".format(epoch))\n",
    "        \n",
    "def print_metrics(y, preds):\n",
    "    print(f'F1 : {f1_score(y, preds)}')\n",
    "    print(f'Accuracy: {accuracy_score(y, preds)}')\n",
    "\n",
    "def plot_outputs(X, y, title=None):\n",
    "    plt.scatter(X[y==1,0],X[y==1,1], c=\"r\", marker=\"+\")\n",
    "    plt.scatter(X[y==0,0],X[y==0,1], c=\"g\", marker=\"o\")\n",
    "    plt.xlabel(\"x1\")\n",
    "    plt.ylabel(\"x2\")\n",
    "    plt.legend([\"1\",\"0\"],loc=0)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_boundary_frontier(X, y, title=None, h=0.02):\n",
    "    cmap_light = ListedColormap(['#B8FFCB', '#FFAAAA'])\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    Z = nn.transform(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.round().reshape(xx.shape)\n",
    "    plt.figure()\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plot_outputs(X, y, title)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-defense",
   "metadata": {},
   "source": [
    "# Define dataset and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "black-kazakhstan",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define features and label\n",
    "X = df.iloc[:, :-1].to_numpy()\n",
    "y = df.iloc[:, -1]\n",
    "y = y.values.reshape(y.shape[0], 1)\n",
    "\n",
    "# Split train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Standardize\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train = sc.transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# Define model's layers\n",
    "hidden_layer_sizes = [200, 100]\n",
    "layers = [\n",
    "    Layer(input_length=X_train.shape[1], n_neurons=hidden_layer_sizes[0]),\n",
    "    Layer(input_length=hidden_layer_sizes[0], n_neurons=hidden_layer_sizes[1]),\n",
    "    Layer(input_length=hidden_layer_sizes[1], n_neurons=1),\n",
    "]\n",
    "\n",
    "# Instantiate model\n",
    "epochs = 10000\n",
    "\n",
    "nn = NeuralNet(\n",
    "    *layers, learning_rate=0.01, epochs=epochs,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "nn.fit(X_train, y_train)\n",
    "nn_preds = nn.transform(X_test)\n",
    "nn_preds_round = nn_preds.round()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brief-geography",
   "metadata": {},
   "source": [
    "## Train and evaluate sklearn models for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encouraging-burden",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = LogisticRegression()\n",
    "cls.fit(X_train, y_train.ravel())\n",
    "lr_preds = cls.predict(X_test)\n",
    "\n",
    "cls = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, max_iter=epochs)\n",
    "cls.fit(X_train, y_train.ravel())\n",
    "mlp_preds = cls.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-house",
   "metadata": {},
   "source": [
    "# Evaluate models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-salmon",
   "metadata": {},
   "source": [
    "#### Real Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "played-corrections",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_outputs(X_test, y_test.ravel(), title='Real labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regular-management",
   "metadata": {},
   "source": [
    "#### NeuralNet Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cognitive-episode",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(y_test, nn_preds_round)\n",
    "plot_outputs(X_test, nn_preds_round.ravel(), title=\"Group's NeuralNet predictions\")\n",
    "plot_boundary_frontier(X_test, y_test.ravel(), title=\"Final Boundary Frontier Comparing With Test Points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "small-apparel",
   "metadata": {},
   "source": [
    "#### Sklearn LogisticRegression predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impossible-certification",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(y_test.ravel(), lr_preds)\n",
    "plot_outputs(X_test, lr_preds, title=\"Sklearn's LogisticRegression predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confident-automation",
   "metadata": {},
   "source": [
    "#### Sklearn MLPClassifier predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respected-atmosphere",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_metrics(y_test.ravel(), mlp_preds)\n",
    "plot_outputs(X_test, mlp_preds, title=\"Sklearn's MLPClassifier predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twelve-inspiration",
   "metadata": {},
   "source": [
    "#### Fronteira de decisão não linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heavy-destruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "new = [it for sublist in list(itertools.chain.from_iterable(nn.weights_matrix)) for it in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-tours",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.weights_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggressive-tobago",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.weights_matrix.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acknowledged-observation",
   "metadata": {},
   "source": [
    "#### Calculo do gradient check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dimensional-quantity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(X, y, nn, epsilon=1e-7):\n",
    "    i=random.randint(0, X.shape[0]-1)\n",
    "    X=np.reshape(np.array([X[i]]),(1,2))\n",
    "    hx = nn.transform(X)\n",
    "    nn.backpropagate(X, hx, y)\n",
    "    \n",
    "    J_plus = cost_not_sum(X+epsilon, y)\n",
    "    J_minus = cost_not_sum(X-epsilon, y)\n",
    "    \n",
    "    numeric_gradient=(j_plus - j_minus)/2*epsilon\n",
    "    \n",
    "    numerator = norm(nn.der_weights_matrix - numeric_gradient)\n",
    "    denominator = norm(numeric_gradient) + norm(nn.der_weights_matrix)\n",
    "    difference = numerator / denominator\n",
    "    print(difference)\n",
    "    \n",
    "def cost_not_sum(self, X, y):\n",
    "    hx = self.transform(X)\n",
    "        \n",
    "    log1_hx = np.log(hx)\n",
    "    log2_hx = np.log(1 - hx)\n",
    "        \n",
    "    y_1 = y * log1_hx\n",
    "    y_0 = (1 - y) * log2_hx\n",
    "\n",
    "    return y_0+y_1\n",
    "\n",
    "gradient_check(X_train, y_train, nn)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
